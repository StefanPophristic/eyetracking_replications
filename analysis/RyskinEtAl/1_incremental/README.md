
# TO DO
I (Stefan) have not had a chance to go through the analysis scripts of run 3. They should be using the data as indicated in this readme in the sections below. The output of the analysis script from run 3 should be sanity checked to make sure that the correlations do make sense (and thus the data from the original eye-tracking experiment and our incremental study are in fact from the same stimuli).

# Intro

Ryskin et al had a counterbalancing error, whereby all participants in the **pragmatic reliable** condition were assigned to list 2 and participants in the **pragmatic unreliable** condition were assigned to list 1. This was misreported in their paper as the opposite (pragmatic reliable condition with list 1, and unreliable condition with list 2).

Three runs of the incremental decision task were conducted. The first two had errors. The final analysis makes use of data from run 2 and 3. For a description of which stimuli were presented, please open the **All_Run_stims.xlsx** file. In this file, the stimuli for the OG (eye-tracking) experiment along with Run 2 and Run 3 of the eyetracking are coded for: whether an adjective was used to describe the presented screen, and whether a contrasting object to the target was presented on the screen.

The **Incremental Task Summary** keynote explains the excel file and the simuli in a bit more detail.

## Run 1
This is the version of the study that was headed by Casey Butcher and was run during the Summer of 2021. The analysis for run 1 is for this experiment, and the poster in the output folder is likewise for this experiment.

This replication of Ryskin et al.'s design, as an incremental design, did not replicate the original study on two counts:
1. The study assigned both **pragmatic reliable** and **unreliable condition** participants to just list 1. This assignment method is what Ryskin et al. originally intended, but not what they did. However, participants were never assigned to list 2.

2. The training trials were miscoded, such that the all training trials for the pragmatic reliable condition appeared with a visually presented contrast set, whereas the pragmatic unreliable condition appeared with no visually presented contrast set.

## Run 2
This is the version of the study that was headed by Stefan Pophristic in the Spring of 2022. The run_2 folder is associated with data from this version of the experiment. The experiment was run such that both pragmatic conditions (**reliable** and **unreliable**) were run on both list 1 and list 2 in order to get the complete set of data.

The **training trials** for the **pragmatic unreliable** condition were miscoded, such that they were identical to the **pragmatic reliable** training trials. Since the **filler trials** are different across pragmatic conditions, this data cannot simply be used as **pragmatic reliable** data.

## Run 3
This is the version of the study that was headed by Stefan Pophristic in the Spring of 2022. The run_3 folder is associated with data from this version of the experiment. This experiment was run to make up for the error in run 2. That is, this experiment was only run for the **pragmatic unreliable** condition on both lists. All of the data in this run was run without mistakes.


# Final analysis

The data that replicates the stimuli from Ryskin et al.'s original experiment are:
- Run 2, **pragmatic reliable**, list 2 data
- Run 3, **pragmatic unreliable**, list 1 data
